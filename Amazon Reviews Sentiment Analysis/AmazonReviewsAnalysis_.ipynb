{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AmazonReviewsAnalysis .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY5G5XhNr61G"
      },
      "source": [
        "**Initialization**\n",
        "* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a Project or Problem. And the third line of code helps to make visualization within the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZYLr4FJqQcv"
      },
      "source": [
        "#@ Initialization: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joCK5JUWsqK1"
      },
      "source": [
        "**Downloading the Libraries and Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQnfGZQlsTGK",
        "outputId": "25c78e1b-9da6-42de-faf5-7f70200abf2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies:\n",
        "from __future__ import absolute_import, division\n",
        "from __future__ import print_function, unicode_literals\n",
        "from IPython.display import display\n",
        "\n",
        "# try:\n",
        "#   !pip uninstall tb-nightly tensorboardX tenosrboard\n",
        "#   !pip install tf-nightly\n",
        "# except Exception:\n",
        "#   pass \n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import datetime\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from keras.models import Sequential                                              # Base Keras Neural Network Model.\n",
        "from keras.layers import Dense, Bidirectional, LSTM, Dropout, Embedding          # Keras Functional API.\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi2tNKekvRT-",
        "outputId": "9dffa1cf-55dc-412f-8cad-3bbc53ef6535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies:\n",
        "import pkg_resources\n",
        "for entry_point in pkg_resources.iter_entry_points(\"tensorboard_plugins\"):\n",
        "  print(entry_point.dist)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorboard 2.3.0\n",
            "tensorboard-plugin-wit 1.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztd_qONmwkB2"
      },
      "source": [
        "#@ Uninstalling the Tensorboard Colab:\n",
        "# !rm -r /usr/local/lib/python3.6/dist-packages/tensorboardcolab-0.0.22.dist-info"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-NbPl35ydlb"
      },
      "source": [
        "**Getting the Data**\n",
        "* I have used Google Colab for this Project so the process of downloading and reading the Data might be different in other platforms. I will use **Amazon Reviews Mobile Electronics Dataset** for this Project and I will import the Dataset using Tensorflow. The Dataset is already present in Tensorflow Dataset Library Corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDrTmCrhyHZb",
        "outputId": "05842604-4b1d-49e6-f1b4-c7e40f9c168e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "#@ Getting the Dataset:\n",
        "dataset, info = tfds.load(\"amazon_us_reviews/Mobile_Electronics_v1_00\", with_info=True)\n",
        "train_dataset = dataset[\"train\"]\n",
        "\n",
        "#@ Inspecting the Information of the Dataset:\n",
        "display(info)\n",
        "\n",
        "#@ Inspecting the Dataset:\n",
        "display(train_dataset)\n",
        "display(len(list(train_dataset)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='amazon_us_reviews',\n",
              "    version=0.1.0,\n",
              "    description='Amazon Customer Reviews (a.k.a. Product Reviews) is one of Amazons iconic products. In a period of over two decades since the first review in 1995, millions of Amazon customers have contributed over a hundred million reviews to express opinions and describe their experiences regarding products on the Amazon.com website. This makes Amazon Customer Reviews a rich source of information for academic researchers in the fields of Natural Language Processing (NLP), Information Retrieval (IR), and Machine Learning (ML), amongst others. Accordingly, we are releasing this data to further research in multiple disciplines related to understanding customer product experiences. Specifically, this dataset was constructed to represent a sample of customer evaluations and opinions, variation in the perception of a product across geographical regions, and promotional intent or bias in reviews.\n",
              "\n",
              "Over 130+ million customer reviews are available to researchers as part of this release. The data is available in TSV files in the amazon-reviews-pds S3 bucket in AWS US East Region. Each line in the data files corresponds to an individual review (tab delimited, with no quote and escape characters).\n",
              "\n",
              "Each Dataset contains the following columns : \n",
              "  marketplace       - 2 letter country code of the marketplace where the review was written.\n",
              "  customer_id       - Random identifier that can be used to aggregate reviews written by a single author.\n",
              "  review_id         - The unique ID of the review.\n",
              "  product_id        - The unique Product ID the review pertains to. In the multilingual dataset the reviews\n",
              "                      for the same product in different countries can be grouped by the same product_id.\n",
              "  product_parent    - Random identifier that can be used to aggregate reviews for the same product.\n",
              "  product_title     - Title of the product.\n",
              "  product_category  - Broad product category that can be used to group reviews \n",
              "                      (also used to group the dataset into coherent parts).\n",
              "  star_rating       - The 1-5 star rating of the review.\n",
              "  helpful_votes     - Number of helpful votes.\n",
              "  total_votes       - Number of total votes the review received.\n",
              "  vine              - Review was written as part of the Vine program.\n",
              "  verified_purchase - The review is on a verified purchase.\n",
              "  review_headline   - The title of the review.\n",
              "  review_body       - The review text.\n",
              "  review_date       - The date the review was written.\n",
              "',\n",
              "    homepage='https://s3.amazonaws.com/amazon-reviews-pds/readme.html',\n",
              "    features=FeaturesDict({\n",
              "        'data': FeaturesDict({\n",
              "            'customer_id': tf.string,\n",
              "            'helpful_votes': tf.int32,\n",
              "            'marketplace': tf.string,\n",
              "            'product_category': tf.string,\n",
              "            'product_id': tf.string,\n",
              "            'product_parent': tf.string,\n",
              "            'product_title': tf.string,\n",
              "            'review_body': tf.string,\n",
              "            'review_date': tf.string,\n",
              "            'review_headline': tf.string,\n",
              "            'review_id': tf.string,\n",
              "            'star_rating': tf.int32,\n",
              "            'total_votes': tf.int32,\n",
              "            'verified_purchase': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "            'vine': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "        }),\n",
              "    }),\n",
              "    total_num_examples=104975,\n",
              "    splits={\n",
              "        'train': 104975,\n",
              "    },\n",
              "    supervised_keys=None,\n",
              "    citation=\"\"\"\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: {data: {customer_id: (), helpful_votes: (), marketplace: (), product_category: (), product_id: (), product_parent: (), product_title: (), review_body: (), review_date: (), review_headline: (), review_id: (), star_rating: (), total_votes: (), verified_purchase: (), vine: ()}}, types: {data: {customer_id: tf.string, helpful_votes: tf.int32, marketplace: tf.string, product_category: tf.string, product_id: tf.string, product_parent: tf.string, product_title: tf.string, review_body: tf.string, review_date: tf.string, review_headline: tf.string, review_id: tf.string, star_rating: tf.int32, total_votes: tf.int32, verified_purchase: tf.int64, vine: tf.int64}}>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "104975"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSd2ogqL2vz5"
      },
      "source": [
        "**Processing the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtR7WNmX1gic",
        "outputId": "4d5daf96-e624-49cc-b716-471e466f848d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "#@ Parameters for Training the Dataset:\n",
        "BUFFER_SIZE = 30000                       # Maximum number of elements that will be buffered when Prefetching.\n",
        "BATCH_SIZE = 128                          # Number of samples fed into the Network.\n",
        "\n",
        "#@ Processing the Dataset:\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=False)  # Shuffling the Dataset.\n",
        "\n",
        "#@ Inspecting the Dataset using Tensorflow:\n",
        "for reviews in train_dataset.take(10):\n",
        "  review_text = reviews[\"data\"]                                                     # \"data\" is the key of Dataset.\n",
        "  print(review_text.get(\"review_body\").numpy())                                     # Converting the Tensors into Numpy arrays.\n",
        "  print(review_text.get(\"star_rating\"))                                             # Inspecting the Ratings.\n",
        "  print(tf.where(review_text.get(\"star_rating\")>3,1,0).numpy())                     # Rating greater than 3 is 1 and else 0."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"Purchased this unit for a weekend camping trip. Charged the unit by USB initially, since I didn't have 13 hours to solar charge ahead of time. Although quite bulky, unit was able to power my T-Mobile Amaze once (full charge) and a G1 3/4 charge. Probably would have been better if I had turned off my phone during charging instead of leaving it on; probably drained more charge that way. My only concern is durability, but only time will tell...\"\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "1\n",
            "b\"This is the third Boxwave case I have purchased for the Nook (and I have purchased Boxwave cases for other devices) and while I LOVE the ones I have received so far this newest one is UNUSABLE because of the overpowering smell of the adhesive used for the backing. I have had it for 5 days now and after 2 days OUTSIDE and 3 days sitting near a window at the expense of my heating bill the smell is still unbearable when on the device. I don't know if they rushed the production to keep up with Holiday orders or if they changed adhesives but this is going back...    If it wasn't for the smell it seams like a good case though it is very hard to get the Nook into it (the bottom straps are just a little to small). This is again something that I have not seen with my other cases leading me to believe they let QC go to get the Holiday orders out. Very disappointing after the other cases.\"\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "0\n",
            "b\"I had one of these and it worked great, but we gave that GPS and mat to my daughter when she relocated.  Since the store didn't have the same GPS mat, I selected a Bracketron Nav-Mat to replace it.  It was thinner, lighter, and slipped off the dash every time we took a turn.  There is nothing so distracting as driving and having something come crashing off your dashboard.  One car trip with that baby and it went bye bye.  I bought this online and it stays put, like a good little GPS mat.  Hallelujah.\"\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "1\n",
            "b'Nice quality'\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "1\n",
            "b\"Y'all did fine came on time, only issue is pioneer no gps showing on the screen or pandora\"\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "1\n",
            "b'Perfect!'\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "1\n",
            "b'Nice packaging - great assortment of cases!'\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "1\n",
            "b\"the built-in FM transmitter doesn't quite grab the desired sound quality.<br /><br />WHY NOT 5-Stars??<br /><br />I have a 2005 Ford Freestar. The factory audio system is already 7.5 out of 10. But playing audio over the air drops the sound quality to under 3. And take it from me, Mr. Low-Maintenance. I don't need two 10s in a speaker box and tweeters and all that jazz; but give me at least the quality I'm starting with--sheeeeesh. My poor kids don't get to hear Dora at her bare-minimum. And while I'm on the subject, no matter what radio station I set the unit to, the signal goes out after 20 mins. Not good at all. The FM transmitter sucks. I need it wired into the radio.<br /><br />SO...<br /><br />I believe this issue will be alleviated once I install either a new head unit (obviously right;]) or, get a fresh antenna. Stay tuned for updates, I'll let you know how that goes.<br /><br />WHY 4-STARS??<br /><br />The picture is PRISTINE. In the brightest daylight, we have a beautiful picture. Doesn't matter if we're watching Friday the 13th or Sesame Street. The video output is wonderful. In fact, at night, you'll be the sensation of the road. The screen illuminates brightly if you need it to and your passengers will be glowing like Martians. Eye-popping almost. It's definitely worth the $$$ if you desire a picture that's resistant to heavy glare and big enough to entertain the masses in your truck/van.<br /><br />The dome lights don't seem bright during the day, but at night, again - you'll be in the NASA Space Station trust me. And the good news is that they're not blinding the driver, just a comfortable bright cool white glow so your passengers can see what the hell their doing (hopefully behaving).<br /><br />The remote is big, but it's flat and all of the buttons are the same size including the volumes. Which means you need to familiarize with where things are in case you're the one setting up picture while driving (highly NOT recommended).<br /><br />This jokerreally is a big screen. Don't be slick and get caught up playing the X-rated stuff. The cops will see it about 10 miles out. Not that I'd know lol. But I'm just sayin, the screen is big.<br /><br />The gray unit is seamless to the color swatches found in my van. So it's a natural fit which outsiders cannot see when I exit the car (unless I leave my screen down).<br /><br />Overall, I really like this unit. You will too.\"\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "1\n",
            "b\"Pros  1. Worth as in price.  2. Nice thin and clear.  3. Alien with iphone home button and front camera.  4. It is includes a cleaning cloth and a small application card (To remove bubbles).    Cons:  1. Easily noticeable Rainbow effect.  (One big disappointment)  2. Not totally scratch guard.  3. Few small bubbles at the edges. (You should perfect to avoid this)    Overall it is nice product for basic Screen Protector. It will be nice to have back protector too.   As earlier suggest by someone     (David H: One tip for installation on any of these protectors. Do it in the bathroom with the shower running. It takes all the dust out of the air, and you'll have a installation without dust under the screen.)    Thank you\"\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "1\n",
            "b'I bought this camera after checking some picture on this product.<br />But the picture and current camera is differnce, for example, AV-OUT is unavailable...<br />I hope to change the picture soon.'\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMnOR_E573Jb"
      },
      "source": [
        "**Preprocessing the Model**\n",
        "* Now, I will Tokenize the Data and convert it into Vocabulary. Vocabulary is the different combinations of words present inside the Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRZpwZOX7PTo",
        "outputId": "2dbb2f67-21ca-4355-9dcc-c78485e83bee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@ Preprocessing the Model: Tokenization:\n",
        "tokenizer = tfds.features.text.Tokenizer()                                         # Instantiating the Tokenizer.\n",
        "\n",
        "vocabulary_set = set()                                                             # Removing the duplicates present in the Dataset.\n",
        "for _, reviews in train_dataset.enumerate():\n",
        "  review_text = reviews[\"data\"]                                                    # \"data\" is the key of the Dataset.\n",
        "  reviews_tokens = tokenizer.tokenize(review_text.get(\"review_body\").numpy())      # Tokenizing the body of the Dataset.\n",
        "  vocabulary_set.update(reviews_tokens)\n",
        "\n",
        "#@ Inspecting the Vocabulary:\n",
        "vocab_size = len(vocabulary_set)                                                   # Inspecting the length or size of Vocabulary.\n",
        "vocab_size "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73738"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9sxz88n_0vB"
      },
      "source": [
        "* Now, I will encode the Vocabulary set into numerical values. I will implement the Text Encoder which basically takes all the Tokenized words and assign it to particular numerical values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ojvoew5_aFX",
        "outputId": "dfca7f93-3382-4e69-fb44-c035b71d7c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "#@ Preprocessing the Model: Encoding:\n",
        "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)                       # Encoding the Vocabulary set. \n",
        "   \n",
        "for reviews in train_dataset.take(10):\n",
        "  review_text = reviews[\"data\"]                                                     # \"data\" is the key of the Dataset.\n",
        "  print(review_text.get(\"review_body\").numpy())\n",
        "  encoded_example = encoder.encode(review_text.get(\"review_body\").numpy())          # Encoding the Dataset.\n",
        "  print(encoded_example)                                                            # Inspecting the Encoded Dataset."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"Purchased this unit for a weekend camping trip. Charged the unit by USB initially, since I didn't have 13 hours to solar charge ahead of time. Although quite bulky, unit was able to power my T-Mobile Amaze once (full charge) and a G1 3/4 charge. Probably would have been better if I had turned off my phone during charging instead of leaving it on; probably drained more charge that way. My only concern is durability, but only time will tell...\"\n",
            "[29929, 60000, 14092, 30618, 17285, 55238, 30077, 59482, 20922, 9736, 14092, 25431, 63661, 21334, 26227, 71324, 43425, 60158, 31155, 47528, 8305, 63524, 52404, 55874, 36375, 67363, 44517, 51708, 50827, 70629, 14092, 39983, 39617, 63524, 36233, 20624, 25332, 2999, 27730, 1108, 42085, 55874, 9830, 17285, 50305, 28335, 15280, 55874, 7063, 70763, 31155, 56174, 24166, 72255, 71324, 18059, 73556, 15911, 20624, 39124, 15800, 11819, 9136, 67363, 15503, 4208, 12304, 57212, 1038, 21213, 55874, 23269, 7091, 44360, 9737, 9648, 2842, 70617, 21165, 9737, 44517, 11589, 37241]\n",
            "b\"This is the third Boxwave case I have purchased for the Nook (and I have purchased Boxwave cases for other devices) and while I LOVE the ones I have received so far this newest one is UNUSABLE because of the overpowering smell of the adhesive used for the backing. I have had it for 5 days now and after 2 days OUTSIDE and 3 days sitting near a window at the expense of my heating bill the smell is still unbearable when on the device. I don't know if they rushed the production to keep up with Holiday orders or if they changed adhesives but this is going back...    If it wasn't for the smell it seams like a good case though it is very hard to get the Nook into it (the bottom straps are just a little to small). This is again something that I have not seen with my other cases leading me to believe they let QC go to get the Holiday orders out. Very disappointing after the other cases.\"\n",
            "[2082, 2842, 9736, 45650, 42477, 13934, 71324, 31155, 21155, 30618, 9736, 36971, 9830, 71324, 31155, 21155, 42477, 29563, 30618, 24971, 32441, 9830, 39508, 71324, 39788, 9736, 35850, 71324, 31155, 29272, 55691, 46417, 60000, 62858, 25655, 2842, 37016, 64324, 67363, 9736, 65991, 3102, 67363, 9736, 67311, 10117, 30618, 9736, 13770, 71324, 31155, 18059, 4208, 30618, 61200, 73453, 56438, 9830, 57045, 55079, 73453, 29227, 9830, 28335, 73453, 62659, 20382, 17285, 28864, 47410, 9736, 26064, 67363, 20624, 33579, 19615, 9736, 3102, 2842, 33996, 1484, 8018, 12304, 9736, 59393, 71324, 28393, 60158, 51631, 72255, 22524, 45295, 9736, 13125, 63524, 65890, 70284, 50966, 43964, 23493, 1497, 72255, 22524, 66362, 24576, 21165, 60000, 2842, 7244, 51155, 6432, 4208, 43243, 60158, 30618, 9736, 3102, 4208, 56775, 48313, 17285, 71762, 13934, 18395, 4208, 2842, 14214, 25547, 63524, 63429, 9736, 36971, 62486, 4208, 9736, 52970, 40153, 22382, 41257, 17285, 64757, 63524, 69138, 2082, 2842, 70942, 17940, 23269, 71324, 31155, 4954, 12579, 50966, 20624, 24971, 29563, 12809, 59211, 63524, 23319, 22524, 46683, 45246, 49348, 63524, 63429, 9736, 43964, 23493, 44356, 15734, 5143, 57045, 9736, 24971, 29563]\n",
            "b\"I had one of these and it worked great, but we gave that GPS and mat to my daughter when she relocated.  Since the store didn't have the same GPS mat, I selected a Bracketron Nav-Mat to replace it.  It was thinner, lighter, and slipped off the dash every time we took a turn.  There is nothing so distracting as driving and having something come crashing off your dashboard.  One car trip with that baby and it went bye bye.  I bought this online and it stays put, like a good little GPS mat.  Hallelujah.\"\n",
            "[71324, 18059, 25655, 67363, 43347, 9830, 4208, 40540, 27013, 21165, 27174, 47042, 23269, 12717, 9830, 29764, 63524, 20624, 7382, 8018, 12974, 70206, 41581, 9736, 11666, 43425, 60158, 31155, 9736, 17373, 12717, 29764, 71324, 45066, 17285, 71083, 5261, 989, 63524, 14329, 4208, 18670, 39983, 34918, 994, 9830, 15353, 15911, 9736, 46807, 51995, 44517, 27174, 39277, 17285, 22517, 69270, 2842, 62436, 55691, 43329, 34754, 15915, 9830, 28525, 17940, 64486, 3359, 15911, 56206, 57179, 27540, 3063, 59482, 50966, 23269, 34115, 9830, 4208, 59592, 39828, 39828, 71324, 66944, 60000, 34465, 9830, 4208, 8576, 55038, 48313, 17285, 71762, 64757, 12717, 29764, 6626]\n",
            "b'Nice quality'\n",
            "[57100, 42114]\n",
            "b\"Y'all did fine came on time, only issue is pioneer no gps showing on the screen or pandora\"\n",
            "[38141, 16977, 28555, 4785, 72606, 12304, 44517, 9737, 71511, 2842, 51813, 69658, 68304, 55634, 12304, 9736, 55797, 1497, 64528]\n",
            "b'Perfect!'\n",
            "[2721]\n",
            "b'Nice packaging - great assortment of cases!'\n",
            "[57100, 69845, 27013, 18291, 67363, 29563]\n",
            "b\"the built-in FM transmitter doesn't quite grab the desired sound quality.<br /><br />WHY NOT 5-Stars??<br /><br />I have a 2005 Ford Freestar. The factory audio system is already 7.5 out of 10. But playing audio over the air drops the sound quality to under 3. And take it from me, Mr. Low-Maintenance. I don't need two 10s in a speaker box and tweeters and all that jazz; but give me at least the quality I'm starting with--sheeeeesh. My poor kids don't get to hear Dora at her bare-minimum. And while I'm on the subject, no matter what radio station I set the unit to, the signal goes out after 20 mins. Not good at all. The FM transmitter sucks. I need it wired into the radio.<br /><br />SO...<br /><br />I believe this issue will be alleviated once I install either a new head unit (obviously right;]) or, get a fresh antenna. Stay tuned for updates, I'll let you know how that goes.<br /><br />WHY 4-STARS??<br /><br />The picture is PRISTINE. In the brightest daylight, we have a beautiful picture. Doesn't matter if we're watching Friday the 13th or Sesame Street. The video output is wonderful. In fact, at night, you'll be the sensation of the road. The screen illuminates brightly if you need it to and your passengers will be glowing like Martians. Eye-popping almost. It's definitely worth the $$$ if you desire a picture that's resistant to heavy glare and big enough to entertain the masses in your truck/van.<br /><br />The dome lights don't seem bright during the day, but at night, again - you'll be in the NASA Space Station trust me. And the good news is that they're not blinding the driver, just a comfortable bright cool white glow so your passengers can see what the hell their doing (hopefully behaving).<br /><br />The remote is big, but it's flat and all of the buttons are the same size including the volumes. Which means you need to familiarize with where things are in case you're the one setting up picture while driving (highly NOT recommended).<br /><br />This jokerreally is a big screen. Don't be slick and get caught up playing the X-rated stuff. The cops will see it about 10 miles out. Not that I'd know lol. But I'm just sayin, the screen is big.<br /><br />The gray unit is seamless to the color swatches found in my van. So it's a natural fit which outsiders cannot see when I exit the car (unless I leave my screen down).<br /><br />Overall, I really like this unit. You will too.\"\n",
            "[9736, 42125, 16820, 54921, 59162, 54262, 60158, 50827, 16760, 9736, 4313, 52555, 42114, 19371, 19371, 61859, 67847, 61200, 51897, 19371, 19371, 71324, 31155, 17285, 44391, 66755, 48499, 30827, 66199, 62683, 62812, 2842, 73261, 40360, 61200, 44356, 67363, 16683, 56324, 62803, 62683, 73482, 9736, 8495, 55265, 9736, 52555, 42114, 63524, 24266, 28335, 11376, 27313, 4208, 3978, 59211, 17233, 59670, 42406, 71324, 28393, 60158, 31213, 5059, 51254, 16820, 17285, 5890, 69066, 9830, 25298, 9830, 16977, 23269, 19316, 21165, 3371, 59211, 47410, 70161, 9736, 42114, 71324, 20578, 140, 50966, 66239, 44360, 61438, 7977, 28393, 60158, 63429, 63524, 53133, 38199, 47410, 61907, 45995, 47441, 11376, 39508, 71324, 20578, 12304, 9736, 66144, 69658, 751, 67772, 31448, 39302, 71324, 37743, 9736, 14092, 63524, 9736, 11273, 41469, 44356, 57045, 23969, 55322, 40536, 71762, 47410, 16977, 30827, 54921, 59162, 35431, 71324, 31213, 4208, 51795, 62486, 9736, 31448, 19371, 19371, 43250, 19371, 19371, 71324, 23319, 60000, 71511, 11589, 33188, 6692, 1108, 71324, 8758, 52891, 17285, 23287, 14853, 14092, 6862, 53384, 1497, 63429, 17285, 16580, 59964, 19952, 47078, 30618, 70514, 71324, 20214, 46683, 40682, 51631, 71085, 23269, 41469, 19371, 19371, 61859, 15280, 27646, 19371, 19371, 30827, 69943, 2842, 20172, 47855, 9736, 33453, 8370, 27174, 31155, 17285, 1240, 69943, 362, 60158, 751, 72255, 27174, 48799, 45509, 5787, 9736, 28498, 1497, 45383, 49539, 30827, 71117, 46084, 2842, 31540, 47855, 16286, 47410, 9173, 40682, 20214, 33188, 9736, 49262, 67363, 9736, 41790, 30827, 55797, 15080, 16992, 72255, 40682, 31213, 4208, 63524, 9830, 56206, 19758, 11589, 33188, 34213, 48313, 32493, 3260, 66468, 40159, 18670, 23175, 71722, 2882, 9736, 72255, 40682, 29628, 17285, 69943, 23269, 23175, 37967, 63524, 65361, 22733, 9830, 38128, 38593, 63524, 56694, 9736, 62306, 16820, 56206, 39764, 66800, 19371, 19371, 30827, 6153, 7739, 28393, 60158, 61197, 41368, 15800, 9736, 58562, 21165, 47410, 9173, 70942, 40682, 20214, 33188, 16820, 9736, 22670, 7687, 70157, 72864, 59211, 11376, 9736, 71762, 15063, 2842, 23269, 22524, 48799, 4954, 69866, 9736, 63235, 41257, 17285, 56335, 41368, 17781, 4719, 45022, 55691, 56206, 19758, 65209, 65603, 67772, 9736, 63586, 40920, 39947, 42513, 18032, 19371, 19371, 30827, 71506, 2842, 38128, 21165, 4208, 23175, 32149, 9830, 16977, 67363, 9736, 22386, 22382, 9736, 17373, 48746, 63891, 9736, 69399, 62325, 15181, 40682, 31213, 63524, 62288, 50966, 58084, 7750, 22382, 16820, 13934, 40682, 48799, 9736, 25655, 28455, 70284, 69943, 39508, 15915, 29005, 67847, 69760, 19371, 19371, 2082, 24703, 2842, 17285, 38128, 55797, 44013, 60158, 33188, 10295, 9830, 63429, 8991, 70284, 62803, 9736, 26176, 44408, 31290, 30827, 8468, 11589, 65603, 4208, 53501, 16683, 19239, 44356, 40536, 23269, 71324, 4546, 51631, 53443, 56324, 71324, 20578, 41257, 70323, 9736, 55797, 2842, 38128, 19371, 19371, 30827, 21954, 14092, 2842, 14746, 63524, 9736, 24668, 16017, 44498, 16820, 20624, 66800, 14531, 4208, 23175, 17285, 48811, 52364, 56450, 36009, 72013, 65603, 8018, 71324, 16723, 9736, 3063, 52167, 71324, 930, 20624, 55797, 49574, 19371, 19371, 67007, 71324, 19300, 48313, 60000, 14092, 4115, 11589, 39621]\n",
            "b\"Pros  1. Worth as in price.  2. Nice thin and clear.  3. Alien with iphone home button and front camera.  4. It is includes a cleaning cloth and a small application card (To remove bubbles).    Cons:  1. Easily noticeable Rainbow effect.  (One big disappointment)  2. Not totally scratch guard.  3. Few small bubbles at the edges. (You should perfect to avoid this)    Overall it is nice product for basic Screen Protector. It will be nice to have back protector too.   As earlier suggest by someone     (David H: One tip for installation on any of these protectors. Do it in the bathroom with the shower running. It takes all the dust out of the air, and you'll have a installation without dust under the screen.)    Thank you\"\n",
            "[19231, 59354, 55242, 34754, 16820, 45685, 55079, 57100, 49863, 9830, 27181, 28335, 4298, 50966, 14074, 13302, 39919, 9830, 69693, 3446, 15280, 18670, 2842, 5174, 17285, 14055, 69287, 9830, 17285, 69138, 22107, 60942, 14789, 62268, 62263, 6504, 59354, 56290, 41006, 8690, 68051, 27540, 38128, 23117, 55079, 40536, 51932, 16863, 33993, 28335, 52269, 69138, 62263, 47410, 9736, 7856, 4115, 1023, 32711, 63524, 58870, 60000, 67007, 4208, 2842, 10262, 59507, 30618, 73177, 50660, 60622, 18670, 11589, 33188, 10262, 63524, 31155, 51155, 66582, 39621, 32465, 1150, 65546, 25431, 48598, 12531, 73121, 27540, 61978, 30618, 47087, 12304, 57211, 67363, 43347, 47980, 11393, 4208, 16820, 9736, 5463, 50966, 9736, 37759, 67097, 18670, 22233, 16977, 9736, 53812, 44356, 67363, 9736, 8495, 9830, 40682, 20214, 31155, 17285, 47087, 28111, 53812, 24266, 9736, 55797, 8049, 40682]\n",
            "b'I bought this camera after checking some picture on this product.<br />But the picture and current camera is differnce, for example, AV-OUT is unavailable...<br />I hope to change the picture soon.'\n",
            "[71324, 66944, 60000, 3446, 57045, 57525, 72810, 69943, 12304, 60000, 59507, 19371, 56324, 9736, 69943, 9830, 63715, 3446, 2842, 10436, 30618, 10899, 38235, 47966, 2842, 55909, 19371, 71324, 3857, 63524, 58302, 9736, 69943, 68405]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFj0LajhDEau",
        "outputId": "7bf54f8b-e771-40ef-873a-90c87a4e6eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "#@ Inspecting the Encoding:\n",
        "for index in encoded_example:\n",
        "  print(\"{} ----> {}\".format(index, encoder.decode([index])))                  # Inspecting in one particular Example."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71324 ----> I\n",
            "66944 ----> bought\n",
            "60000 ----> this\n",
            "3446 ----> camera\n",
            "57045 ----> after\n",
            "57525 ----> checking\n",
            "72810 ----> some\n",
            "69943 ----> picture\n",
            "12304 ----> on\n",
            "60000 ----> this\n",
            "59507 ----> product\n",
            "19371 ----> br\n",
            "56324 ----> But\n",
            "9736 ----> the\n",
            "69943 ----> picture\n",
            "9830 ----> and\n",
            "63715 ----> current\n",
            "3446 ----> camera\n",
            "2842 ----> is\n",
            "10436 ----> differnce\n",
            "30618 ----> for\n",
            "10899 ----> example\n",
            "38235 ----> AV\n",
            "47966 ----> OUT\n",
            "2842 ----> is\n",
            "55909 ----> unavailable\n",
            "19371 ----> br\n",
            "71324 ----> I\n",
            "3857 ----> hope\n",
            "63524 ----> to\n",
            "58302 ----> change\n",
            "9736 ----> the\n",
            "69943 ----> picture\n",
            "68405 ----> soon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aChpqMwcGDc7"
      },
      "source": [
        "**Preprocessing the Model**\n",
        "* Now, I will Encode all the Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2djQ6xPEH0B",
        "outputId": "344a812a-6979-43ed-ce7a-974d30a07eac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "#@ Preprocessing the Model:\n",
        "def encode(text_tensor, label_tensor):\n",
        "  encoded_text = encoder.encode(text_tensor.numpy())        \n",
        "  label = tf.where(label_tensor > 3,1,0)\n",
        "  return encoded_text, label\n",
        "\n",
        "def encode_map_fn(tensor):\n",
        "  text = tensor[\"data\"].get(\"review_body\")                        # Accessing the review body from Tensor.\n",
        "  label = tensor[\"data\"].get(\"star_rating\")                        # Accessing the ratings from Tensor.\n",
        "  encoded_text, label = tf.py_function(encode,\n",
        "                                       inp=[text, label],\n",
        "                                       Tout=(tf.int64, tf.int32))\n",
        "  encoded_text.set_shape([None])                                   # Automatically takes the shape.\n",
        "  label.set_shape([])                                              # Automatically takes the shape.\n",
        "  return encoded_text, label\n",
        "\n",
        "#@ Encoding the Dataset:\n",
        "encoded_data = train_dataset.map(encode_map_fn)                    # Encoding all the Dataset.\n",
        "\n",
        "#@ Inspecting the Encoded Dataset:\n",
        "for f0, f1 in encoded_data.take(3):\n",
        "  print(f0)                                                        # Encoded text review.\n",
        "  print(f1)                                                        # Encoded label."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[29929 60000 14092 30618 17285 55238 30077 59482 20922  9736 14092 25431\n",
            " 63661 21334 26227 71324 43425 60158 31155 47528  8305 63524 52404 55874\n",
            " 36375 67363 44517 51708 50827 70629 14092 39983 39617 63524 36233 20624\n",
            " 25332  2999 27730  1108 42085 55874  9830 17285 50305 28335 15280 55874\n",
            "  7063 70763 31155 56174 24166 72255 71324 18059 73556 15911 20624 39124\n",
            " 15800 11819  9136 67363 15503  4208 12304 57212  1038 21213 55874 23269\n",
            "  7091 44360  9737  9648  2842 70617 21165  9737 44517 11589 37241], shape=(83,), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(\n",
            "[ 2082  2842  9736 45650 42477 13934 71324 31155 21155 30618  9736 36971\n",
            "  9830 71324 31155 21155 42477 29563 30618 24971 32441  9830 39508 71324\n",
            " 39788  9736 35850 71324 31155 29272 55691 46417 60000 62858 25655  2842\n",
            " 37016 64324 67363  9736 65991  3102 67363  9736 67311 10117 30618  9736\n",
            " 13770 71324 31155 18059  4208 30618 61200 73453 56438  9830 57045 55079\n",
            " 73453 29227  9830 28335 73453 62659 20382 17285 28864 47410  9736 26064\n",
            " 67363 20624 33579 19615  9736  3102  2842 33996  1484  8018 12304  9736\n",
            " 59393 71324 28393 60158 51631 72255 22524 45295  9736 13125 63524 65890\n",
            " 70284 50966 43964 23493  1497 72255 22524 66362 24576 21165 60000  2842\n",
            "  7244 51155  6432  4208 43243 60158 30618  9736  3102  4208 56775 48313\n",
            " 17285 71762 13934 18395  4208  2842 14214 25547 63524 63429  9736 36971\n",
            " 62486  4208  9736 52970 40153 22382 41257 17285 64757 63524 69138  2082\n",
            "  2842 70942 17940 23269 71324 31155  4954 12579 50966 20624 24971 29563\n",
            " 12809 59211 63524 23319 22524 46683 45246 49348 63524 63429  9736 43964\n",
            " 23493 44356 15734  5143 57045  9736 24971 29563], shape=(176,), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(\n",
            "[71324 18059 25655 67363 43347  9830  4208 40540 27013 21165 27174 47042\n",
            " 23269 12717  9830 29764 63524 20624  7382  8018 12974 70206 41581  9736\n",
            " 11666 43425 60158 31155  9736 17373 12717 29764 71324 45066 17285 71083\n",
            "  5261   989 63524 14329  4208 18670 39983 34918   994  9830 15353 15911\n",
            "  9736 46807 51995 44517 27174 39277 17285 22517 69270  2842 62436 55691\n",
            " 43329 34754 15915  9830 28525 17940 64486  3359 15911 56206 57179 27540\n",
            "  3063 59482 50966 23269 34115  9830  4208 59592 39828 39828 71324 66944\n",
            " 60000 34465  9830  4208  8576 55038 48313 17285 71762 64757 12717 29764\n",
            "  6626], shape=(97,), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjXsc4qgL3w8"
      },
      "source": [
        "**Splitting the Dataset**\n",
        "* Now, The Text Dataset is Tokenized and Encoded into particular Integers and it's readily available for Training the Model. So, I will split the Dataset into Training set and Testing or Validation set. I will split 10000 Encoded Dataset for Testing and the remaining Encoded Dataset for Training the Model. I will also perform padding to the Dataset to make the Data of constant length. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6VKJzyeKas2",
        "outputId": "1424b602-d46c-4d50-bb05-072621b15494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@ Splitting the Dataset:\n",
        "TAKE_SIZE = 10000\n",
        "\n",
        "#@ Training Dataset:\n",
        "train_data = encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)           # Skipping the 10000 Encoded Dataset. \n",
        "train_data = train_data.padded_batch(BATCH_SIZE)                         # Padding the Dataset for constant length.\n",
        "\n",
        "#@ Testing or Validation Dataset:\n",
        "test_data = encoded_data.take(TAKE_SIZE)\n",
        "test_data = test_data.padded_batch(BATCH_SIZE)                           # Padding the Dataset for constant length.\n",
        "\n",
        "#@ Inspecting the Test Data:\n",
        "sample_text, sample_label = next(iter(test_data))\n",
        "sample_text[0], sample_label[0]                                          # Inspecting the Test Data."
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(676,), dtype=int64, numpy=\n",
              " array([29929, 60000, 14092, 30618, 17285, 55238, 30077, 59482, 20922,\n",
              "         9736, 14092, 25431, 63661, 21334, 26227, 71324, 43425, 60158,\n",
              "        31155, 47528,  8305, 63524, 52404, 55874, 36375, 67363, 44517,\n",
              "        51708, 50827, 70629, 14092, 39983, 39617, 63524, 36233, 20624,\n",
              "        25332,  2999, 27730,  1108, 42085, 55874,  9830, 17285, 50305,\n",
              "        28335, 15280, 55874,  7063, 70763, 31155, 56174, 24166, 72255,\n",
              "        71324, 18059, 73556, 15911, 20624, 39124, 15800, 11819,  9136,\n",
              "        67363, 15503,  4208, 12304, 57212,  1038, 21213, 55874, 23269,\n",
              "         7091, 44360,  9737,  9648,  2842, 70617, 21165,  9737, 44517,\n",
              "        11589, 37241,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1QbVxf8QpCY",
        "outputId": "4f55e528-9be3-4d18-e972-4f4a7af986ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#@ Inspecting the Distribution of Positive and Negative Reviews:\n",
        "for f0, f1 in test_data.take(10):\n",
        "  print(tf.unique_with_counts(f1)[2].numpy())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[96 32]\n",
            "[44 84]\n",
            "[44 84]\n",
            "[86 42]\n",
            "[42 86]\n",
            "[96 32]\n",
            "[86 42]\n",
            "[47 81]\n",
            "[84 44]\n",
            "[40 88]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozRq3hc8Xjda"
      },
      "source": [
        "* The Distribution of positive and negative sentiment reviews are not equal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqYpqgufZJ-y"
      },
      "source": [
        "**Long Short Term Memory**\n",
        "* Long Short Term Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of Deep Learning. Unlike standard Feedforward Neural Networks, LSTM has Feedback connections. It can not only process single data points, but also entire sequences of data such as Speech or Video. Now, The Dataset is ready to build the Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4ExsgPEXQ0W",
        "outputId": "19cb82f1-661f-49e5-b23d-f983cc61bdba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#@ Long Short Term Memory or LSTM:\n",
        "vocab_size = vocab_size + 1                                   # Increasing the vocab size by 1.\n",
        "\n",
        "model = Sequential()                                          # Standard Model Definition for Keras.\n",
        "model.add(Embedding(                                          # Adding the Embedding Layer.\n",
        "    vocab_size, 128     \n",
        "))\n",
        "model.add(Bidirectional(LSTM(                                 # Adding the Bidirectional LSTM Layer.\n",
        "    128, return_sequences=True\n",
        ")))\n",
        "model.add(Bidirectional(LSTM(                                 # Adding another LSTM Layer.\n",
        "    64, return_sequences=True\n",
        ")))\n",
        "model.add(Bidirectional(LSTM(                                 # Adding the third LSTM Layer.\n",
        "    64, return_sequences=False\n",
        ")))\n",
        "model.add(Dense(                                              # Adding the Dense Layer\n",
        "    64, activation=\"relu\"\n",
        "))\n",
        "model.add(Dense(                                              # Adding another Dense Layer.\n",
        "    64, activation=\"relu\"\n",
        "))\n",
        "model.add(Dense(\n",
        "    1, activation=\"sigmoid\"                                   # Adding the Output Layer.\n",
        "))\n",
        "\n",
        "#@ Inspecting the Summary of the Model:\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 128)         9438592   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 256)         263168    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, None, 128)         164352    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 9,977,409\n",
            "Trainable params: 9,977,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3oHtk14g-K8"
      },
      "source": [
        "# #@ Processing the Model:\n",
        "# !rm -r /tmp/logs/                                                                     # Cleaning the tmp logs.\n",
        "\n",
        "logdir = os.path.join(\"/tmp/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))   # Creating the log directory.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)         # Captures the progress of the Model.\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"/tmp/sentiment.hdf5\", verbose=1, save_best_only=True                      # Saves only the best Model.\n",
        ")\n",
        "\n",
        "# #@ Compiling the LSTM Neural Network:\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# #@ Training the LSTM Neural Network:\n",
        "history = model.fit(\n",
        "    train_data, epochs=2,\n",
        "    validation_data=test_data,\n",
        "    callbacks=[tensorboard_callback, checkpointer]\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}